//! # Integration Tests for Data Flow Coordination System\n//!\n//! This module provides comprehensive integration tests for the complete data\n//! flow coordination system, testing coordination across all existing modules,\n//! real-time performance requirements, error recovery, backpressure handling,\n//! and ensuring no performance regression.\n\nuse std::sync::{Arc, RwLock, Mutex};\nuse std::collections::HashMap;\nuse std::time::{Instant, Duration};\nuse std::sync::mpsc::{self, Sender, Receiver};\n\nuse super::*;\nuse crate::modules::application_core::*;\nuse crate::modules::audio_foundations::*;\nuse crate::modules::platform_abstraction::*;\n\n/// Integration test suite for data flow coordination\npub struct DataFlowIntegrationTestSuite {\n    /// Data flow coordinator under test\n    coordinator: DataFlowCoordinatorImpl,\n    /// Audio data pipeline for real-time testing\n    audio_pipeline: AudioDataPipeline,\n    /// Data transformer for format conversion testing\n    data_transformer: DataTransformer,\n    /// Backpressure controller for flow control testing\n    backpressure_controller: BackpressureController,\n    /// Flow metrics collector for monitoring testing\n    metrics_collector: FlowMetricsCollector,\n    /// Flow recovery manager for resilience testing\n    recovery_manager: FlowRecoveryManager,\n    /// Test configuration\n    test_config: IntegrationTestConfig,\n    /// Test results storage\n    test_results: Arc<RwLock<Vec<TestResult>>>,\n}\n\n/// Integration test configuration\n#[derive(Debug, Clone)]\npub struct IntegrationTestConfig {\n    pub performance_test_duration_ms: u64,\n    pub target_throughput_ops_per_second: u32,\n    pub target_latency_ms: f32,\n    pub max_error_rate_percentage: f32,\n    pub stress_test_multiplier: u32,\n    pub recovery_test_failure_rate: f32,\n}\n\nimpl Default for IntegrationTestConfig {\n    fn default() -> Self {\n        Self {\n            performance_test_duration_ms: 10000, // 10 seconds\n            target_throughput_ops_per_second: 1000,\n            target_latency_ms: 2.0,\n            max_error_rate_percentage: 1.0,\n            stress_test_multiplier: 5,\n            recovery_test_failure_rate: 0.1,\n        }\n    }\n}\n\n/// Test result for individual test cases\n#[derive(Debug, Clone)]\npub struct TestResult {\n    pub test_name: String,\n    pub success: bool,\n    pub execution_time_ms: f32,\n    pub throughput_achieved: f32,\n    pub latency_achieved: f32,\n    pub error_rate: f32,\n    pub details: String,\n    pub performance_regression: bool,\n}\n\n/// Performance baseline for regression testing\n#[derive(Debug, Clone)]\npub struct PerformanceBaseline {\n    pub baseline_throughput: f32,\n    pub baseline_latency: f32,\n    pub baseline_error_rate: f32,\n    pub baseline_timestamp: std::time::SystemTime,\n}\n\nimpl Default for PerformanceBaseline {\n    fn default() -> Self {\n        Self {\n            baseline_throughput: 1000.0,\n            baseline_latency: 2.0,\n            baseline_error_rate: 0.5,\n            baseline_timestamp: std::time::SystemTime::now(),\n        }\n    }\n}\n\nimpl DataFlowIntegrationTestSuite {\n    /// Create a new integration test suite\n    pub fn new() -> Self {\n        Self {\n            coordinator: DataFlowCoordinatorImpl::new(),\n            audio_pipeline: AudioDataPipeline::new(),\n            data_transformer: DataTransformer::new(),\n            backpressure_controller: BackpressureController::new(),\n            metrics_collector: FlowMetricsCollector::new(),\n            recovery_manager: FlowRecoveryManager::new(),\n            test_config: IntegrationTestConfig::default(),\n            test_results: Arc::new(RwLock::new(Vec::new())),\n        }\n    }\n    \n    /// Run complete integration test suite\n    pub fn run_complete_test_suite(&mut self) -> IntegrationTestSummary {\n        let start_time = Instant::now();\n        \n        // Test 1: Basic data flow coordination\n        self.test_basic_data_flow_coordination();\n        \n        // Test 2: Cross-module integration\n        self.test_cross_module_integration();\n        \n        // Test 3: Real-time performance requirements\n        self.test_real_time_performance_requirements();\n        \n        // Test 4: Data transformation accuracy\n        self.test_data_transformation_accuracy();\n        \n        // Test 5: Backpressure handling under load\n        self.test_backpressure_handling();\n        \n        // Test 6: Error recovery and resilience\n        self.test_error_recovery_resilience();\n        \n        // Test 7: Performance regression testing\n        self.test_performance_regression();\n        \n        // Test 8: Stress testing\n        self.test_system_under_stress();\n        \n        // Test 9: Memory and resource management\n        self.test_memory_resource_management();\n        \n        // Test 10: End-to-end workflow testing\n        self.test_end_to_end_workflows();\n        \n        let total_time = start_time.elapsed();\n        self.generate_test_summary(total_time)\n    }\n    \n    /// Test basic data flow coordination functionality\n    fn test_basic_data_flow_coordination(&mut self) {\n        let test_start = Instant::now();\n        let test_name = \"basic_data_flow_coordination\";\n        \n        let result = self.run_test_case(test_name, || {\n            // Register pipeline\n            let pipeline_id = self.coordinator.register_pipeline(\n                ModuleId::AudioFoundations,\n                ModuleId::DataManagement,\n                PipelineConfig::default(),\n            )?;\n            \n            // Start coordinator\n            self.coordinator.start()?;\n            \n            // Send test data\n            let test_data = FlowData {\n                format: DataFormat::AudioBuffer,\n                data: vec![1, 2, 3, 4],\n                metadata: HashMap::new(),\n                timestamp: Instant::now(),\n            };\n            \n            self.coordinator.send_data(pipeline_id.clone(), test_data)?;\n            \n            // Verify pipeline health\n            let health = self.coordinator.get_pipeline_health(pipeline_id);\n            assert_eq!(health, PipelineHealth::Healthy);\n            \n            Ok(())\n        });\n        \n        self.record_test_result(test_name, result, test_start.elapsed());\n    }\n    \n    /// Test cross-module integration\n    fn test_cross_module_integration(&mut self) {\n        let test_start = Instant::now();\n        let test_name = \"cross_module_integration\";\n        \n        let result = self.run_test_case(test_name, || {\n            // Test Audio Foundations integration\n            let audio_pipeline_id = self.audio_pipeline.register_audio_pipeline(\n                ModuleId::AudioFoundations,\n                ModuleId::DataManagement,\n                AudioDataFormat::F32Array,\n                RealtimeConstraints::default(),\n            )?;\n            \n            // Test Platform Abstraction integration\n            let platform_pipeline_id = self.coordinator.register_pipeline(\n                ModuleId::PlatformAbstraction,\n                ModuleId::DataManagement,\n                PipelineConfig {\n                    buffer_size: 1024,\n                    max_latency_ms: 5.0,\n                    priority: PipelinePriority::Normal,\n                    backpressure_strategy: BackpressureStrategy::Throttle,\n                    transformation_required: false,\n                },\n            )?;\n            \n            // Start all systems\n            self.coordinator.start()?;\n            self.audio_pipeline.start()?;\n            \n            // Send data through both pipelines\n            let audio_data = vec![0.1, 0.2, 0.3, 0.4];\n            let audio_metadata = AudioBufferMetadata {\n                sample_rate: 44100.0,\n                channels: 2,\n                buffer_size: 1024,\n                timestamp: 12345,\n                sequence_number: 1,\n                latency_budget_ms: 2.0,\n            };\n            \n            self.audio_pipeline.send_audio_buffer(\n                audio_pipeline_id,\n                audio_data,\n                audio_metadata,\n            )?;\n            \n            let platform_data = FlowData {\n                format: DataFormat::ConfigurationData,\n                data: vec![5, 6, 7, 8],\n                metadata: HashMap::new(),\n                timestamp: Instant::now(),\n            };\n            \n            self.coordinator.send_data(platform_pipeline_id, platform_data)?;\n            \n            // Verify both pipelines are healthy\n            assert_eq!(self.coordinator.get_pipeline_health(audio_pipeline_id), PipelineHealth::Healthy);\n            assert_eq!(self.coordinator.get_pipeline_health(platform_pipeline_id), PipelineHealth::Healthy);\n            \n            Ok(())\n        });\n        \n        self.record_test_result(test_name, result, test_start.elapsed());\n    }\n    \n    /// Test real-time performance requirements\n    fn test_real_time_performance_requirements(&mut self) {\n        let test_start = Instant::now();\n        let test_name = \"real_time_performance\";\n        \n        let result = self.run_test_case(test_name, || {\n            // Set up high-performance pipeline\n            let pipeline_id = self.coordinator.register_pipeline(\n                ModuleId::AudioFoundations,\n                ModuleId::DataManagement,\n                PipelineConfig {\n                    buffer_size: 2048,\n                    max_latency_ms: 2.0,\n                    priority: PipelinePriority::Critical,\n                    backpressure_strategy: BackpressureStrategy::Drop,\n                    transformation_required: false,\n                },\n            )?;\n            \n            self.coordinator.start()?;\n            \n            // Performance test parameters\n            let test_duration = Duration::from_millis(self.test_config.performance_test_duration_ms);\n            let target_ops = self.test_config.target_throughput_ops_per_second;\n            let target_latency = self.test_config.target_latency_ms;\n            \n            let performance_start = Instant::now();\n            let mut operations_completed = 0u32;\n            let mut total_latency = 0.0f32;\n            let mut errors = 0u32;\n            \n            // Run performance test\n            while performance_start.elapsed() < test_duration {\n                let op_start = Instant::now();\n                \n                let test_data = FlowData {\n                    format: DataFormat::AudioBuffer,\n                    data: vec![0u8; 1024], // 1KB test data\n                    metadata: HashMap::new(),\n                    timestamp: Instant::now(),\n                };\n                \n                match self.coordinator.send_data(pipeline_id.clone(), test_data) {\n                    Ok(_) => {\n                        operations_completed += 1;\n                        total_latency += op_start.elapsed().as_millis() as f32;\n                    },\n                    Err(_) => errors += 1,\n                }\n                \n                // Small delay to control rate\n                std::thread::sleep(Duration::from_micros(100));\n            }\n            \n            let test_duration_seconds = performance_start.elapsed().as_secs_f32();\n            let actual_throughput = operations_completed as f32 / test_duration_seconds;\n            let average_latency = if operations_completed > 0 {\n                total_latency / operations_completed as f32\n            } else {\n                f32::MAX\n            };\n            let error_rate = (errors as f32 / (operations_completed + errors) as f32) * 100.0;\n            \n            // Record performance metrics for this test\n            self.record_performance_metrics(actual_throughput, average_latency, error_rate);\n            \n            // Validate performance requirements\n            assert!(\n                actual_throughput >= target_ops as f32 * 0.8, // Allow 20% tolerance\n                \"Throughput {} < target {}\", actual_throughput, target_ops\n            );\n            assert!(\n                average_latency <= target_latency * 1.5, // Allow 50% tolerance\n                \"Latency {} > target {}\", average_latency, target_latency\n            );\n            assert!(\n                error_rate <= self.test_config.max_error_rate_percentage,\n                \"Error rate {} > max {}\", error_rate, self.test_config.max_error_rate_percentage\n            );\n            \n            Ok(())\n        });\n        \n        self.record_test_result(test_name, result, test_start.elapsed());\n    }\n    \n    /// Test data transformation accuracy\n    fn test_data_transformation_accuracy(&mut self) {\n        let test_start = Instant::now();\n        let test_name = \"data_transformation_accuracy\";\n        \n        let result = self.run_test_case(test_name, || {\n            // Test audio buffer transformations\n            let audio_data = FlowData {\n                format: DataFormat::AudioBuffer,\n                data: vec![0, 0, 0, 0, 0, 0, 128, 63], // f32 1.0 as bytes\n                metadata: {\n                    let mut meta = HashMap::new();\n                    meta.insert(\"sample_rate\".to_string(), \"44100\".to_string());\n                    meta.insert(\"channels\".to_string(), \"2\".to_string());\n                    meta\n                },\n                timestamp: Instant::now(),\n            };\n            \n            // Test transformation to binary data\n            let result = self.data_transformer.transform_data(\n                audio_data.clone(),\n                DataFormat::AudioBuffer,\n                DataFormat::BinaryData,\n            )?;\n            \n            assert_eq!(result.data.format, DataFormat::BinaryData);\n            assert!(result.metadata_preserved);\n            assert!(result.bytes_processed > 0);\n            \n            // Test round-trip transformation\n            let round_trip = self.data_transformer.transform_data(\n                result.data,\n                DataFormat::BinaryData,\n                DataFormat::AudioBuffer,\n            )?;\n            \n            assert_eq!(round_trip.data.format, DataFormat::AudioBuffer);\n            \n            // Test metadata preservation\n            assert!(round_trip.data.metadata.contains_key(\"sample_rate\"));\n            assert!(round_trip.data.metadata.contains_key(\"channels\"));\n            \n            // Test zero-copy optimization when possible\n            let same_format_result = self.data_transformer.transform_data(\n                audio_data,\n                DataFormat::AudioBuffer,\n                DataFormat::AudioBuffer,\n            )?;\n            \n            assert!(same_format_result.zero_copy_used);\n            assert_eq!(same_format_result.transform_time_ms, 0.0);\n            \n            Ok(())\n        });\n        \n        self.record_test_result(test_name, result, test_start.elapsed());\n    }\n    \n    /// Test backpressure handling under load\n    fn test_backpressure_handling(&mut self) {\n        let test_start = Instant::now();\n        let test_name = \"backpressure_handling\";\n        \n        let result = self.run_test_case(test_name, || {\n            // Register pipeline with backpressure monitoring\n            let pipeline_id = PipelineId(\"test_backpressure\".to_string());\n            let config = FlowControlConfig {\n                pipeline_id: pipeline_id.clone(),\n                buffer_threshold: 0.8,\n                latency_threshold_ms: 5.0,\n                throughput_threshold: 500.0,\n                strategy: BackpressureStrategy::Throttle,\n                max_retries: 3,\n                recovery_timeout_ms: 1000,\n                adaptive_enabled: true,\n            };\n            \n            self.backpressure_controller.register_pipeline(pipeline_id.clone(), config)?;\n            \n            // Simulate high congestion\n            let congestion_metrics = CongestionMetrics {\n                buffer_utilization: 0.95, // High utilization\n                latency_ms: 10.0,         // High latency\n                throughput_ops_per_sec: 200.0, // Low throughput\n                queue_length: 1500,       // High queue length\n                drop_rate: 0.05,\n            };\n            \n            // Detect backpressure\n            let congestion_level = self.backpressure_controller.detect_backpressure(\n                &pipeline_id,\n                congestion_metrics,\n            )?;\n            \n            assert_ne!(congestion_level, CongestionLevel::None);\n            \n            // Apply backpressure strategy\n            let backpressure_result = self.backpressure_controller.apply_backpressure(\n                &pipeline_id,\n                BackpressureStrategy::Throttle,\n            )?;\n            \n            assert!(backpressure_result.success);\n            assert!(backpressure_result.recovery_time_ms > 0.0);\n            \n            // Test adaptive adjustment\n            self.backpressure_controller.adapt_flow_control(&pipeline_id)?;\n            \n            // Verify metrics were updated\n            let metrics = self.backpressure_controller.get_metrics()\n                .ok_or(FlowError::InvalidConfiguration)?;\n            assert!(metrics.total_backpressure_events > 0);\n            \n            Ok(())\n        });\n        \n        self.record_test_result(test_name, result, test_start.elapsed());\n    }\n    \n    /// Test error recovery and resilience\n    fn test_error_recovery_resilience(&mut self) {\n        let test_start = Instant::now();\n        let test_name = \"error_recovery_resilience\";\n        \n        let result = self.run_test_case(test_name, || {\n            let pipeline_id = PipelineId(\"test_recovery\".to_string());\n            \n            // Register recovery strategy\n            let recovery_strategy = RecoveryStrategy {\n                pipeline_id: pipeline_id.clone(),\n                strategy_type: RecoveryStrategyType::Retry,\n                max_retry_attempts: 3,\n                initial_backoff_ms: 100,\n                max_backoff_ms: 5000,\n                backoff_multiplier: 2.0,\n                recovery_timeout_ms: 30000,\n                fallback_strategy: None,\n                integrity_check_required: true,\n                graceful_degradation_enabled: true,\n            };\n            \n            self.recovery_manager.register_recovery_strategy(pipeline_id.clone(), recovery_strategy)?;\n            \n            // Simulate various failure scenarios\n            let failures = vec![\n                FlowError::LatencyExceeded,\n                FlowError::BackpressureExceeded,\n                FlowError::TransformationFailed,\n            ];\n            \n            for failure in failures {\n                let operation_id = self.recovery_manager.detect_and_recover(\n                    &pipeline_id,\n                    failure,\n                    None,\n                )?;\n                \n                assert!(!operation_id.is_empty());\n                \n                // Wait for recovery to complete (simulated)\n                std::thread::sleep(Duration::from_millis(100));\n                \n                // Verify recovery metrics\n                let metrics = self.recovery_manager.get_recovery_metrics()\n                    .ok_or(FlowError::RecoveryFailed)?;\n                assert!(metrics.total_recovery_attempts > 0);\n            }\n            \n            // Test graceful degradation\n            let degradation_operation = self.recovery_manager.detect_and_recover(\n                &pipeline_id,\n                FlowError::RecoveryFailed,\n                Some({\n                    let mut context = HashMap::new();\n                    context.insert(\"degradation_requested\".to_string(), \"true\".to_string());\n                    context\n                }),\n            )?;\n            \n            assert!(!degradation_operation.is_empty());\n            \n            Ok(())\n        });\n        \n        self.record_test_result(test_name, result, test_start.elapsed());\n    }\n    \n    /// Test performance regression\n    fn test_performance_regression(&mut self) {\n        let test_start = Instant::now();\n        let test_name = \"performance_regression\";\n        \n        let baseline = PerformanceBaseline::default();\n        \n        let result = self.run_test_case(test_name, || {\n            // Run performance test and compare to baseline\n            let current_metrics = self.run_performance_benchmark()?;\n            \n            let throughput_regression = (baseline.baseline_throughput - current_metrics.0) / baseline.baseline_throughput;\n            let latency_regression = (current_metrics.1 - baseline.baseline_latency) / baseline.baseline_latency;\n            let error_rate_regression = current_metrics.2 - baseline.baseline_error_rate;\n            \n            // Allow up to 5% regression in throughput\n            assert!(\n                throughput_regression <= 0.05,\n                \"Throughput regression {:.2}% exceeds 5%\", throughput_regression * 100.0\n            );\n            \n            // Allow up to 10% regression in latency\n            assert!(\n                latency_regression <= 0.10,\n                \"Latency regression {:.2}% exceeds 10%\", latency_regression * 100.0\n            );\n            \n            // Allow up to 1% increase in error rate\n            assert!(\n                error_rate_regression <= 1.0,\n                \"Error rate regression {:.2}% exceeds 1%\", error_rate_regression\n            );\n            \n            Ok(())\n        });\n        \n        self.record_test_result(test_name, result, test_start.elapsed());\n    }\n    \n    /// Test system under stress\n    fn test_system_under_stress(&mut self) {\n        let test_start = Instant::now();\n        let test_name = \"stress_testing\";\n        \n        let result = self.run_test_case(test_name, || {\n            // Create multiple pipelines for stress testing\n            let mut pipeline_ids = Vec::new();\n            \n            for i in 0..10 {\n                let pipeline_id = self.coordinator.register_pipeline(\n                    ModuleId::AudioFoundations,\n                    ModuleId::DataManagement,\n                    PipelineConfig {\n                        buffer_size: 1024,\n                        max_latency_ms: 5.0,\n                        priority: if i < 3 { PipelinePriority::Critical } else { PipelinePriority::Normal },\n                        backpressure_strategy: BackpressureStrategy::Throttle,\n                        transformation_required: true,\n                    },\n                )?;\n                pipeline_ids.push(pipeline_id);\n            }\n            \n            self.coordinator.start()?;\n            \n            // Stress test with high load\n            let stress_duration = Duration::from_millis(5000); // 5 seconds\n            let stress_start = Instant::now();\n            let mut total_operations = 0u32;\n            let mut total_errors = 0u32;\n            \n            while stress_start.elapsed() < stress_duration {\n                for pipeline_id in &pipeline_ids {\n                    let stress_data = FlowData {\n                        format: DataFormat::AudioBuffer,\n                        data: vec![0u8; 2048], // 2KB stress data\n                        metadata: HashMap::new(),\n                        timestamp: Instant::now(),\n                    };\n                    \n                    match self.coordinator.send_data(pipeline_id.clone(), stress_data) {\n                        Ok(_) => total_operations += 1,\n                        Err(_) => total_errors += 1,\n                    }\n                }\n                \n                // No delay - maximum stress\n            }\n            \n            let stress_duration_seconds = stress_start.elapsed().as_secs_f32();\n            let stress_throughput = total_operations as f32 / stress_duration_seconds;\n            let stress_error_rate = (total_errors as f32 / (total_operations + total_errors) as f32) * 100.0;\n            \n            // Verify system survived stress test\n            assert!(stress_throughput > 0.0, \"System completely failed under stress\");\n            assert!(\n                stress_error_rate < 20.0, // Allow higher error rate under stress\n                \"Error rate {} too high under stress\", stress_error_rate\n            );\n            \n            // Verify all pipelines are still responding\n            for pipeline_id in &pipeline_ids {\n                let health = self.coordinator.get_pipeline_health(pipeline_id.clone());\n                assert_ne!(health, PipelineHealth::Failed);\n            }\n            \n            Ok(())\n        });\n        \n        self.record_test_result(test_name, result, test_start.elapsed());\n    }\n    \n    /// Test memory and resource management\n    fn test_memory_resource_management(&mut self) {\n        let test_start = Instant::now();\n        let test_name = \"memory_resource_management\";\n        \n        let result = self.run_test_case(test_name, || {\n            // Monitor memory usage during operations\n            let pipeline_id = self.coordinator.register_pipeline(\n                ModuleId::AudioFoundations,\n                ModuleId::DataManagement,\n                PipelineConfig::default(),\n            )?;\n            \n            self.coordinator.start()?;\n            \n            // Send many operations to test memory management\n            for i in 0..1000 {\n                let large_data = FlowData {\n                    format: DataFormat::AudioBuffer,\n                    data: vec![0u8; 10240], // 10KB per operation\n                    metadata: {\n                        let mut meta = HashMap::new();\n                        meta.insert(\"operation_id\".to_string(), i.to_string());\n                        meta\n                    },\n                    timestamp: Instant::now(),\n                };\n                \n                self.coordinator.send_data(pipeline_id.clone(), large_data)?;\n                \n                // Occasional small delay to allow cleanup\n                if i % 100 == 0 {\n                    std::thread::sleep(Duration::from_millis(10));\n                }\n            }\n            \n            // Test that system is still responsive after heavy usage\n            let test_data = FlowData {\n                format: DataFormat::AudioBuffer,\n                data: vec![1, 2, 3, 4],\n                metadata: HashMap::new(),\n                timestamp: Instant::now(),\n            };\n            \n            self.coordinator.send_data(pipeline_id.clone(), test_data)?;\n            \n            let health = self.coordinator.get_pipeline_health(pipeline_id);\n            assert_eq!(health, PipelineHealth::Healthy);\n            \n            Ok(())\n        });\n        \n        self.record_test_result(test_name, result, test_start.elapsed());\n    }\n    \n    /// Test end-to-end workflows\n    fn test_end_to_end_workflows(&mut self) {\n        let test_start = Instant::now();\n        let test_name = \"end_to_end_workflows\";\n        \n        let result = self.run_test_case(test_name, || {\n            // Set up complete data flow pipeline\n            let audio_to_data = self.coordinator.register_pipeline(\n                ModuleId::AudioFoundations,\n                ModuleId::DataManagement,\n                PipelineConfig::default(),\n            )?;\n            \n            let data_to_platform = self.coordinator.register_pipeline(\n                ModuleId::DataManagement,\n                ModuleId::PlatformAbstraction,\n                PipelineConfig::default(),\n            )?;\n            \n            self.coordinator.start()?;\n            \n            // Start metrics collection\n            self.metrics_collector.start_monitoring(audio_to_data.clone());\n            self.metrics_collector.start_monitoring(data_to_platform.clone());\n            \n            // Simulate complete workflow\n            let workflow_data = FlowData {\n                format: DataFormat::AudioBuffer,\n                data: vec![0, 0, 128, 63], // Audio sample data\n                metadata: {\n                    let mut meta = HashMap::new();\n                    meta.insert(\"workflow_id\".to_string(), \"e2e_test_001\".to_string());\n                    meta.insert(\"timestamp\".to_string(), \"12345\".to_string());\n                    meta\n                },\n                timestamp: Instant::now(),\n            };\n            \n            // Send through first pipeline\n            self.coordinator.send_data(audio_to_data.clone(), workflow_data.clone())?;\n            \n            // Transform data for second pipeline\n            let transformed_data = self.data_transformer.transform_data(\n                workflow_data,\n                DataFormat::AudioBuffer,\n                DataFormat::ConfigurationData,\n            )?;\n            \n            // Send through second pipeline\n            self.coordinator.send_data(data_to_platform.clone(), transformed_data.data)?;\n            \n            // Update metrics\n            self.metrics_collector.update_metrics(&audio_to_data, 100.0, 1.5, 0, 0.3);\n            self.metrics_collector.update_metrics(&data_to_platform, 95.0, 2.0, 0, 0.4);\n            \n            // Verify end-to-end metrics\n            let audio_metrics = self.metrics_collector.get_pipeline_metrics(&audio_to_data)\n                .ok_or(FlowError::PipelineNotFound)?;\n            let platform_metrics = self.metrics_collector.get_pipeline_metrics(&data_to_platform)\n                .ok_or(FlowError::PipelineNotFound)?;\n            \n            assert_eq!(audio_metrics.health_metrics.health_status, PipelineHealth::Healthy);\n            assert_eq!(platform_metrics.health_metrics.health_status, PipelineHealth::Healthy);\n            \n            assert!(audio_metrics.throughput_metrics.current_ops_per_second > 0.0);\n            assert!(platform_metrics.throughput_metrics.current_ops_per_second > 0.0);\n            \n            Ok(())\n        });\n        \n        self.record_test_result(test_name, result, test_start.elapsed());\n    }\n    \n    /// Helper method to run a test case and handle errors\n    fn run_test_case<F>(&self, test_name: &str, test_fn: F) -> Result<(), Box<dyn std::error::Error>>\n    where\n        F: FnOnce() -> Result<(), FlowError>,\n    {\n        match test_fn() {\n            Ok(_) => Ok(()),\n            Err(e) => Err(Box::new(std::io::Error::new(\n                std::io::ErrorKind::Other,\n                format!(\"Test {} failed: {:?}\", test_name, e),\n            ))),\n        }\n    }\n    \n    /// Record test result\n    fn record_test_result(&self, test_name: &str, result: Result<(), Box<dyn std::error::Error>>, duration: Duration) {\n        let test_result = TestResult {\n            test_name: test_name.to_string(),\n            success: result.is_ok(),\n            execution_time_ms: duration.as_millis() as f32,\n            throughput_achieved: 0.0, // Set by specific tests\n            latency_achieved: 0.0,    // Set by specific tests\n            error_rate: 0.0,          // Set by specific tests\n            details: if let Err(e) = result {\n                e.to_string()\n            } else {\n                \"Success\".to_string()\n            },\n            performance_regression: false, // Set by regression tests\n        };\n        \n        if let Ok(mut results) = self.test_results.write() {\n            results.push(test_result);\n        }\n    }\n    \n    /// Record performance metrics for a test\n    fn record_performance_metrics(&self, throughput: f32, latency: f32, error_rate: f32) {\n        if let Ok(mut results) = self.test_results.write() {\n            if let Some(last_result) = results.last_mut() {\n                last_result.throughput_achieved = throughput;\n                last_result.latency_achieved = latency;\n                last_result.error_rate = error_rate;\n            }\n        }\n    }\n    \n    /// Run performance benchmark\n    fn run_performance_benchmark(&mut self) -> Result<(f32, f32, f32), FlowError> {\n        let pipeline_id = self.coordinator.register_pipeline(\n            ModuleId::AudioFoundations,\n            ModuleId::DataManagement,\n            PipelineConfig::default(),\n        )?;\n        \n        self.coordinator.start()?;\n        \n        let benchmark_start = Instant::now();\n        let mut operations = 0u32;\n        let mut total_latency = 0.0f32;\n        let mut errors = 0u32;\n        \n        // Run benchmark for 2 seconds\n        while benchmark_start.elapsed() < Duration::from_millis(2000) {\n            let op_start = Instant::now();\n            \n            let test_data = FlowData {\n                format: DataFormat::AudioBuffer,\n                data: vec![0u8; 512],\n                metadata: HashMap::new(),\n                timestamp: Instant::now(),\n            };\n            \n            match self.coordinator.send_data(pipeline_id.clone(), test_data) {\n                Ok(_) => {\n                    operations += 1;\n                    total_latency += op_start.elapsed().as_millis() as f32;\n                },\n                Err(_) => errors += 1,\n            }\n        }\n        \n        let duration_seconds = benchmark_start.elapsed().as_secs_f32();\n        let throughput = operations as f32 / duration_seconds;\n        let average_latency = if operations > 0 {\n            total_latency / operations as f32\n        } else {\n            f32::MAX\n        };\n        let error_rate = (errors as f32 / (operations + errors) as f32) * 100.0;\n        \n        Ok((throughput, average_latency, error_rate))\n    }\n    \n    /// Generate comprehensive test summary\n    fn generate_test_summary(&self, total_duration: Duration) -> IntegrationTestSummary {\n        let results = if let Ok(results) = self.test_results.read() {\n            results.clone()\n        } else {\n            Vec::new()\n        };\n        \n        let total_tests = results.len();\n        let passed_tests = results.iter().filter(|r| r.success).count();\n        let failed_tests = total_tests - passed_tests;\n        \n        let average_throughput = if !results.is_empty() {\n            results.iter().map(|r| r.throughput_achieved).sum::<f32>() / results.len() as f32\n        } else {\n            0.0\n        };\n        \n        let average_latency = if !results.is_empty() {\n            results.iter().map(|r| r.latency_achieved).sum::<f32>() / results.len() as f32\n        } else {\n            0.0\n        };\n        \n        let max_error_rate = results.iter()\n            .map(|r| r.error_rate)\n            .fold(0.0f32, |acc, x| acc.max(x));\n        \n        let performance_regressions = results.iter()\n            .filter(|r| r.performance_regression)\n            .count();\n        \n        IntegrationTestSummary {\n            total_tests,\n            passed_tests,\n            failed_tests,\n            total_duration_ms: total_duration.as_millis() as f32,\n            average_throughput_ops_per_second: average_throughput,\n            average_latency_ms: average_latency,\n            max_error_rate_percentage: max_error_rate,\n            performance_regressions,\n            test_results: results,\n            requirements_met: self.validate_requirements(passed_tests, total_tests, average_throughput, average_latency, max_error_rate),\n        }\n    }\n    \n    /// Validate that all requirements are met\n    fn validate_requirements(\n        &self,\n        passed: usize,\n        total: usize,\n        throughput: f32,\n        latency: f32,\n        error_rate: f32,\n    ) -> bool {\n        let pass_rate = (passed as f32 / total as f32) * 100.0;\n        \n        pass_rate >= 90.0 && // At least 90% tests must pass\n        throughput >= self.test_config.target_throughput_ops_per_second as f32 * 0.8 && // 80% of target throughput\n        latency <= self.test_config.target_latency_ms * 1.5 && // 150% of target latency\n        error_rate <= self.test_config.max_error_rate_percentage * 2.0 // 200% of target error rate\n    }\n}\n\n/// Integration test summary\n#[derive(Debug, Clone)]\npub struct IntegrationTestSummary {\n    pub total_tests: usize,\n    pub passed_tests: usize,\n    pub failed_tests: usize,\n    pub total_duration_ms: f32,\n    pub average_throughput_ops_per_second: f32,\n    pub average_latency_ms: f32,\n    pub max_error_rate_percentage: f32,\n    pub performance_regressions: usize,\n    pub test_results: Vec<TestResult>,\n    pub requirements_met: bool,\n}\n\nimpl IntegrationTestSummary {\n    /// Generate a human-readable test report\n    pub fn generate_report(&self) -> String {\n        let mut report = String::new();\n        \n        report.push_str(\"=== Data Flow Coordination Integration Test Report ===\\n\\n\");\n        \n        report.push_str(&format!(\"Total Tests: {}\\n\", self.total_tests));\n        report.push_str(&format!(\"Passed: {} ({:.1}%)\\n\", \n            self.passed_tests, \n            (self.passed_tests as f32 / self.total_tests as f32) * 100.0\n        ));\n        report.push_str(&format!(\"Failed: {} ({:.1}%)\\n\", \n            self.failed_tests,\n            (self.failed_tests as f32 / self.total_tests as f32) * 100.0\n        ));\n        report.push_str(&format!(\"Total Duration: {:.2}ms\\n\\n\", self.total_duration_ms));\n        \n        report.push_str(\"=== Performance Metrics ===\\n\");\n        report.push_str(&format!(\"Average Throughput: {:.1} ops/sec\\n\", self.average_throughput_ops_per_second));\n        report.push_str(&format!(\"Average Latency: {:.2}ms\\n\", self.average_latency_ms));\n        report.push_str(&format!(\"Max Error Rate: {:.2}%\\n\", self.max_error_rate_percentage));\n        report.push_str(&format!(\"Performance Regressions: {}\\n\\n\", self.performance_regressions));\n        \n        report.push_str(\"=== Requirements Validation ===\\n\");\n        report.push_str(&format!(\"All Requirements Met: {}\\n\\n\", \n            if self.requirements_met { \"✅ YES\" } else { \"❌ NO\" }\n        ));\n        \n        report.push_str(\"=== Individual Test Results ===\\n\");\n        for result in &self.test_results {\n            let status = if result.success { \"✅ PASS\" } else { \"❌ FAIL\" };\n            report.push_str(&format!(\n                \"{} {} - {:.2}ms (T:{:.1} L:{:.2} E:{:.2}%)\\n\",\n                status,\n                result.test_name,\n                result.execution_time_ms,\n                result.throughput_achieved,\n                result.latency_achieved,\n                result.error_rate\n            ));\n            \n            if !result.success {\n                report.push_str(&format!(\"   Error: {}\\n\", result.details));\n            }\n        }\n        \n        report\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    \n    #[test]\n    fn test_integration_test_suite_creation() {\n        let test_suite = DataFlowIntegrationTestSuite::new();\n        assert!(test_suite.test_results.read().unwrap().is_empty());\n    }\n    \n    #[test]\n    fn test_basic_data_flow_coordination() {\n        let mut test_suite = DataFlowIntegrationTestSuite::new();\n        test_suite.test_basic_data_flow_coordination();\n        \n        let results = test_suite.test_results.read().unwrap();\n        assert!(!results.is_empty());\n        assert_eq!(results[0].test_name, \"basic_data_flow_coordination\");\n    }\n    \n    #[test]\n    fn test_performance_benchmark() {\n        let mut test_suite = DataFlowIntegrationTestSuite::new();\n        let result = test_suite.run_performance_benchmark();\n        \n        assert!(result.is_ok());\n        let (throughput, latency, error_rate) = result.unwrap();\n        assert!(throughput >= 0.0);\n        assert!(latency >= 0.0);\n        assert!(error_rate >= 0.0);\n    }\n    \n    #[test]\n    fn test_requirements_validation() {\n        let test_suite = DataFlowIntegrationTestSuite::new();\n        \n        // Test with good metrics\n        assert!(test_suite.validate_requirements(95, 100, 1200.0, 1.5, 0.5));\n        \n        // Test with poor metrics\n        assert!(!test_suite.validate_requirements(50, 100, 100.0, 10.0, 5.0));\n    }\n}"